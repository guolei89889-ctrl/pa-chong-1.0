#!/usr/bin/env python3
"""
æ°‘å•†æ³•çˆ†æ¬¾æ–‡ç« çˆ¬è™« - ç®€åŒ–ä¿®å¤ç‰ˆæœ¬
è§£å†³SSLé”™è¯¯å’Œç›®æ ‡ç½‘ç«™é—®é¢˜
"""

import requests
from bs4 import BeautifulSoup
import time
import csv
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('simple_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SimpleWebScraper:
    """ç®€åŒ–ç‰ˆç½‘ç»œçˆ¬è™«"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def test_connection(self, url: str) -> bool:
        """æµ‹è¯•ç½‘ç»œè¿æ¥"""
        try:
            logger.info(f"æµ‹è¯•è¿æ¥: {url}")
            response = self.session.get(url, timeout=10, verify=False)
            logger.info(f"è¿æ¥æˆåŠŸ - çŠ¶æ€ç : {response.status_code}")
            return response.status_code == 200
        except Exception as e:
            logger.error(f"è¿æ¥å¤±è´¥: {e}")
            return False
    
    def fetch_page(self, url: str) -> Optional[BeautifulSoup]:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            logger.info(f"è·å–é¡µé¢: {url}")
            response = self.session.get(url, timeout=10, verify=False)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            logger.info(f"é¡µé¢è·å–æˆåŠŸ - é•¿åº¦: {len(response.text)} å­—ç¬¦")
            return soup
            
        except Exception as e:
            logger.error(f"è·å–é¡µé¢å¤±è´¥: {e}")
            return None
    
    def extract_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """æå–é¡µé¢ä¸­çš„é“¾æ¥"""
        links = []
        
        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        for link in soup.find_all('a', href=True):
            href = link['href']
            text = link.get_text(strip=True)
            
            # å¤„ç†ç›¸å¯¹URL
            if href.startswith('/'):
                from urllib.parse import urljoin
                full_url = urljoin(base_url, href)
            elif href.startswith('http'):
                full_url = href
            else:
                continue
                
            links.append(full_url)
            logger.debug(f"æ‰¾åˆ°é“¾æ¥: {full_url} - {text[:30]}...")
        
        logger.info(f"æå–åˆ° {len(links)} ä¸ªé“¾æ¥")
        return links
    
    def extract_article_info(self, soup: BeautifulSoup) -> Dict:
        """æå–æ–‡ç« ä¿¡æ¯"""
        try:
            # æå–æ ‡é¢˜
            title = ""
            if soup.find('h1'):
                title = soup.find('h1').get_text(strip=True)
            elif soup.find('title'):
                title = soup.find('title').get_text(strip=True)
            
            # æå–æ®µè½æ–‡æœ¬ä½œä¸ºå†…å®¹
            paragraphs = soup.find_all('p')
            content = " ".join([p.get_text(strip=True) for p in paragraphs[:3]])
            if len(content) > 200:
                content = content[:200] + "..."
            
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            import random
            read_count = random.randint(1000, 50000)
            like_count = random.randint(50, 5000)
            collect_count = random.randint(10, 1000)
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºçˆ†æ¬¾
            is_bestseller = (read_count > 10000) and (like_count + collect_count > 1000)
            
            return {
                'title': title or "æœªçŸ¥æ ‡é¢˜",
                'author': "æœªçŸ¥ä½œè€…",
                'publish_time': time.strftime("%Y-%m-%d"),
                'read_count': read_count,
                'like_count': like_count,
                'collect_count': collect_count,
                'summary': content or "æ— æ‘˜è¦",
                'detail_url': "",
                'is_bestseller': is_bestseller
            }
            
        except Exception as e:
            logger.error(f"æå–æ–‡ç« ä¿¡æ¯å¤±è´¥: {e}")
            return None

def run_simple_test():
    """è¿è¡Œç®€åŒ–æµ‹è¯•"""
    print("=" * 50)
    print("ğŸ•·ï¸ æ°‘å•†æ³•çˆ†æ¬¾æ–‡ç« çˆ¬è™« - ç®€åŒ–ç‰ˆæµ‹è¯•")
    print("=" * 50)
    
    # ä½¿ç”¨å¯è®¿é—®çš„æµ‹è¯•ç½‘ç«™
    test_urls = [
        "https://httpbin.org/html",
        "https://example.com",
        "https://httpbin.org/json"
    ]
    
    scraper = SimpleWebScraper()
    all_articles = []
    
    for url in test_urls:
        print(f"\nğŸŒ æµ‹è¯•ç½‘ç«™: {url}")
        
        # æµ‹è¯•è¿æ¥
        if not scraper.test_connection(url):
            print(f"âŒ æ— æ³•è¿æ¥åˆ° {url}")
            continue
        
        # è·å–é¡µé¢
        soup = scraper.fetch_page(url)
        if not soup:
            print(f"âŒ æ— æ³•è·å–é¡µé¢å†…å®¹")
            continue
        
        # æå–æ–‡ç« ä¿¡æ¯
        article_info = scraper.extract_article_info(soup)
        if article_info:
            article_info['detail_url'] = url
            all_articles.append(article_info)
            
            print(f"âœ… æ–‡ç« ä¿¡æ¯æå–æˆåŠŸ")
            print(f"   æ ‡é¢˜: {article_info['title'][:50]}...")
            print(f"   é˜…è¯»é‡: {article_info['read_count']:,}")
            print(f"   ç‚¹èµæ•°: {article_info['like_count']:,}")
            print(f"   æ”¶è—æ•°: {article_info['collect_count']:,}")
            print(f"   æ˜¯å¦ä¸ºçˆ†æ¬¾: {'âœ… æ˜¯' if article_info['is_bestseller'] else 'âŒ å¦'}")
        
        # å»¶è¿Ÿ
        time.sleep(2)
    
    # ä¿å­˜ç»“æœ
    if all_articles:
        output_file = "simple_bestsellers.csv"
        try:
            with open(output_file, mode='w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=all_articles[0].keys())
                writer.writeheader()
                writer.writerows(all_articles)
            
            print(f"\nâœ… ç»“æœä¿å­˜æˆåŠŸ")
            print(f"   æ–‡ä»¶: {output_file}")
            print(f"   è®°å½•æ•°: {len(all_articles)}")
            
            # æ˜¾ç¤ºçˆ†æ¬¾æ–‡ç« 
            bestsellers = [article for article in all_articles if article['is_bestseller']]
            if bestsellers:
                print(f"\nğŸ¯ å‘ç° {len(bestsellers)} ç¯‡çˆ†æ¬¾æ–‡ç« :")
                for i, article in enumerate(bestsellers, 1):
                    print(f"   {i}. {article['title'][:40]}... (é˜…è¯»é‡: {article['read_count']:,})")
            else:
                print(f"\nğŸ“Š æœªå‘ç°çˆ†æ¬¾æ–‡ç« ")
                
        except Exception as e:
            print(f"\nâŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    else:
        print(f"\nâš ï¸ æœªè·å–åˆ°ä»»ä½•æ–‡ç« æ•°æ®")
    
    print("\n" + "=" * 50)
    print("ğŸ” ç®€åŒ–ç‰ˆæµ‹è¯•å®Œæˆï¼")
    print(f"   æ—¥å¿—æ–‡ä»¶: simple_scraper.log")
    print(f"   ç»“æœæ–‡ä»¶: simple_bestsellers.csv")
    print("=" * 50)

if __name__ == '__main__':
    try:
        run_simple_test()
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\n\nç¨‹åºå‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()