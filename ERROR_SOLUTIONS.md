# ğŸ•·ï¸ æ°‘å•†æ³•çˆ†æ¬¾æ–‡ç« çˆ¬è™« - é”™è¯¯è§£å†³æ–¹æ¡ˆ

## ğŸ” é—®é¢˜è¯Šæ–­

### âŒ å‘ç°çš„é”™è¯¯

1. **SSLè¿æ¥é”™è¯¯**ï¼š`SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol'))`
2. **ç›®æ ‡ç½‘ç«™ä¸å­˜åœ¨**ï¼š`example-law-platform.com` æ˜¯ç¤ºä¾‹åŸŸå
3. **Pythonç¯å¢ƒé—®é¢˜**ï¼šæ§åˆ¶å°ç¼“å†²åŒºå¼‚å¸¸
4. **æ¨¡å—å¯¼å…¥é”™è¯¯**ï¼šç¼ºå°‘å¿…è¦çš„å¯¼å…¥

## âœ… è§£å†³æ–¹æ¡ˆ

### 1. SSLè¿æ¥é”™è¯¯ä¿®å¤

**é—®é¢˜åŸå› **ï¼š
- ç›®æ ‡ç½‘ç«™ä½¿ç”¨HTTPSåè®®ä½†è¯ä¹¦é…ç½®æœ‰é—®é¢˜
- Pythonçš„SSLéªŒè¯è¿‡äºä¸¥æ ¼

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# åœ¨è¯·æ±‚ä¸­æ·»åŠ  verify=False å‚æ•°
response = requests.get(url, timeout=10, verify=False)

# ç¦ç”¨SSLè­¦å‘Š
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
```

### 2. ç›®æ ‡ç½‘ç«™é…ç½®

**é—®é¢˜åŸå› **ï¼š
- ä½¿ç”¨äº†ä¸å­˜åœ¨çš„ç¤ºä¾‹åŸŸå
- éœ€è¦é…ç½®çœŸå®çš„æ³•å¾‹ç½‘ç«™

**è§£å†³æ–¹æ¡ˆ**ï¼š
```json
{
    "target_platform": {
        "base_url": "https://çœŸå®çš„æ³•å¾‹ç½‘ç«™.com/law-section",
        "headers": {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
    }
}
```

### 3. æ¨èæµ‹è¯•ç½‘ç«™

**å¯ç”¨çš„æµ‹è¯•ç½‘ç«™**ï¼š
- `https://httpbin.org/html` - HTTPæµ‹è¯•æœåŠ¡
- `https://example.com` - ç¤ºä¾‹ç½‘ç«™
- `https://httpbin.org/json` - JSONæµ‹è¯•

### 4. å®Œæ•´ä¿®å¤ä»£ç 

**åˆ›å»ºä¿®å¤ç‰ˆæœ¬** (`fixed_scraper.py`)ï¼š
```python
import requests
from bs4 import BeautifulSoup
import time
import csv
import logging
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FixedWebScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def fetch_page(self, url: str) -> BeautifulSoup:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            response = self.session.get(url, timeout=10, verify=False)
            response.raise_for_status()
            return BeautifulSoup(response.text, 'html.parser')
        except Exception as e:
            logger.error(f"è·å–é¡µé¢å¤±è´¥: {e}")
            return None
    
    def extract_data(self, soup: BeautifulSoup) -> dict:
        """æå–æ•°æ®"""
        if not soup:
            return None
        
        # æå–æ ‡é¢˜
        title = soup.find('h1').text if soup.find('h1') else soup.find('title').text if soup.find('title') else "æ— æ ‡é¢˜"
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        import random
        return {
            'title': title,
            'author': 'æµ‹è¯•ä½œè€…',
            'read_count': random.randint(5000, 50000),
            'like_count': random.randint(500, 5000),
            'collect_count': random.randint(100, 1000),
            'is_bestseller': False
        }
    
    def run(self, url: str):
        """è¿è¡Œçˆ¬è™«"""
        logger.info(f"å¼€å§‹çˆ¬å–: {url}")
        
        soup = self.fetch_page(url)
        if soup:
            data = self.extract_data(soup)
            if data:
                # åˆ¤æ–­æ˜¯å¦ä¸ºçˆ†æ¬¾
                data['is_bestseller'] = (data['read_count'] > 10000) and (data['like_count'] + data['collect_count'] > 1000)
                
                # ä¿å­˜åˆ°CSV
                with open('bestsellers.csv', 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=data.keys())
                    writer.writeheader()
                    writer.writerow(data)
                
                logger.info(f"çˆ¬å–å®Œæˆï¼æ ‡é¢˜: {data['title']}, çˆ†æ¬¾: {data['is_bestseller']}")
                return data
        
        return None

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    scraper = FixedWebScraper()
    result = scraper.run('https://httpbin.org/html')
    if result:
        print(f"âœ… çˆ¬å–æˆåŠŸ: {result}")
    else:
        print("âŒ çˆ¬å–å¤±è´¥")
```

## ğŸ§ª æµ‹è¯•éªŒè¯

### å¿«é€Ÿæµ‹è¯•
```bash
# è¿è¡Œä¿®å¤ç‰ˆæœ¬
python fixed_scraper.py

# æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
cat bestsellers.csv
```

### é¢„æœŸç»“æœ
```csv
title,author,read_count,like_count,collect_count,is_bestseller
Herman Melville - Moby-Dick,æµ‹è¯•ä½œè€…,15000,1500,500,True
```

## ğŸ¯ ä½¿ç”¨å»ºè®®

### 1. é…ç½®çœŸå®ç½‘ç«™
```json
{
    "target_platform": {
        "base_url": "https://çœŸå®çš„æ³•å¾‹ç½‘ç«™.com/law-articles",
        "selectors": {
            "article_links": "a.article-link",
            "title": "h1.article-title",
            "author": ".author-name",
            "read_count": ".read-count"
        }
    }
}
```

### 2. é€‰æ‹©å™¨è°ƒè¯•æŠ€å·§
```python
# ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·è·å–é€‰æ‹©å™¨
# 1. æ‰“å¼€ç›®æ ‡ç½‘ç«™
# 2. å³é”®ç‚¹å‡»å…ƒç´  â†’ æ£€æŸ¥
# 3. åœ¨Elementsæ ‡ç­¾ä¸­æ‰¾åˆ°å¯¹åº”å…ƒç´ 
# 4. å³é”® â†’ Copy â†’ Copy selector
```

### 3. åçˆ¬è™«å¯¹ç­–
```python
# å¢åŠ éšæœºå»¶è¿Ÿ
time.sleep(random.uniform(2, 5))

# ä½¿ç”¨ä»£ç†IP
proxies = {'http': 'http://proxy:port', 'https': 'https://proxy:port'}
response = requests.get(url, proxies=proxies)

# è½®æ¢User-Agent
user_agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
]
headers = {'User-Agent': random.choice(user_agents)}
```

## ğŸ“‹ å¸¸è§é—®é¢˜FAQ

### Q: ä¸ºä»€ä¹ˆè¿˜æ˜¯è¿æ¥å¤±è´¥ï¼Ÿ
**A**: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
- ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
- ç›®æ ‡ç½‘ç«™æ˜¯å¦å¯è®¿é—®
- é˜²ç«å¢™æ˜¯å¦é˜»æ­¢äº†è¯·æ±‚
- æ˜¯å¦éœ€è¦ä»£ç†IP

### Q: é€‰æ‹©å™¨æ€»æ˜¯æ— æ•ˆæ€ä¹ˆåŠï¼Ÿ
**A**: 
- ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·ç¡®è®¤é€‰æ‹©å™¨
- æ£€æŸ¥ç½‘ç«™æ˜¯å¦ä½¿ç”¨JavaScriptåŠ¨æ€åŠ è½½å†…å®¹
- è€ƒè™‘ä½¿ç”¨Seleniumç­‰å·¥å…·

### Q: è¢«å°IPäº†æ€ä¹ˆåŠï¼Ÿ
**A**:
- å¢åŠ è¯·æ±‚å»¶è¿Ÿï¼ˆå»ºè®®5-10ç§’ï¼‰
- ä½¿ç”¨ä»£ç†IPè½®æ¢
- é™ä½çˆ¬å–é¢‘ç‡
- æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º

### Q: å¦‚ä½•å¤„ç†JavaScriptæ¸²æŸ“çš„é¡µé¢ï¼Ÿ
**A**:
- ä½¿ç”¨Selenium + WebDriver
- ä½¿ç”¨Pyppeteer
- åˆ†æAPIæ¥å£ç›´æ¥è·å–æ•°æ®

## ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œ

1. **æµ‹è¯•ä¿®å¤ç‰ˆæœ¬**ï¼šè¿è¡Œ `python fixed_scraper.py`
2. **é…ç½®çœŸå®ç½‘ç«™**ï¼šä¿®æ”¹ `config.json` ä¸­çš„URL
3. **è°ƒæ•´é€‰æ‹©å™¨**ï¼šæ ¹æ®ç›®æ ‡ç½‘ç«™ç»“æ„é…ç½®
4. **è®¾ç½®åˆç†å‚æ•°**ï¼šè°ƒæ•´å»¶è¿Ÿå’Œçˆ†æ¬¾æ ‡å‡†
5. **å¼€å§‹æ­£å¼çˆ¬å–**ï¼šä½¿ç”¨ `configurable_scraper.py`

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œè¯·ï¼š
1. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯
2. ç¡®è®¤ç›®æ ‡ç½‘ç«™çš„æœåŠ¡æ¡æ¬¾
3. è€ƒè™‘ä½¿ç”¨æ›´é«˜çº§çš„åçˆ¬è™«æŠ€æœ¯
4. å¯»æ±‚ä¸“ä¸šçš„çˆ¬è™«å¼€å‘å¸®åŠ©

---

**æ³¨æ„**ï¼šæœ¬è§£å†³æ–¹æ¡ˆä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ï¼Œè¯·ç¡®ä¿æ‚¨çš„çˆ¬å–è¡Œä¸ºç¬¦åˆç›®æ ‡ç½‘ç«™çš„æœåŠ¡æ¡æ¬¾å’Œç›¸å…³æ³•å¾‹æ³•è§„ã€‚