#!/usr/bin/env python3
"""
æ°‘å•†æ³•çˆ†æ¬¾æ–‡ç« çˆ¬è™« - ä¿®å¤ç‰ˆæœ¬
è§£å†³SSLé”™è¯¯å’Œç›®æ ‡ç½‘ç«™é—®é¢˜
"""

import requests
from bs4 import BeautifulSoup
import time
import csv
import logging
import json
from typing import List, Dict, Optional
import random
from urllib.parse import urljoin, urlparse
from pathlib import Path
import traceback
import ssl
import urllib3
import os

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('fixed_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FixedConfigManager:
    """ä¿®å¤ç‰ˆé…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_file: str = 'config.json'):
        self.config_file = config_file
        self.config = self.load_config()
        
    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if not os.path.exists(self.config_file):
                logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_file}ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
                return self.get_default_config()
                
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            logger.error(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            return self.get_default_config()
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return self.get_default_config()
    
    def get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½® - ä½¿ç”¨å¯è®¿é—®çš„æµ‹è¯•ç½‘ç«™"""
        return {
            "target_platform": {
                "base_url": "https://httpbin.org/html",  # ä½¿ç”¨æµ‹è¯•ç½‘ç«™
                "headers": {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
                },
                "selectors": {
                    "article_links": "a",  # ç®€åŒ–é€‰æ‹©å™¨
                    "title": "h1",
                    "author": "p",
                    "publish_time": "p",
                    "read_count": "p",
                    "like_count": "p",
                    "collect_count": "p",
                    "summary": "p"
                }
            },
            "scraping": {
                "max_pages": 1,
                "max_retries": 3,
                "retry_delay": 2,
                "request_timeout": 15,  # å¢åŠ è¶…æ—¶æ—¶é—´
                "request_delay_min": 2.0,  # å¢åŠ å»¶è¿Ÿ
                "request_delay_max": 4.0,
                "page_delay_min": 3.0,
                "page_delay_max": 5.0
            },
            "bestseller_criteria": {
                "min_read_count": 100,  # é™ä½æ ‡å‡†ä»¥ä¾¿æµ‹è¯•
                "min_interaction_count": 10
            },
            "output": {
                "csv_filename": "fixed_bestsellers.csv",
                "encoding": "utf-8-sig",
                "log_filename": "fixed_scraper.log"
            },
            "logging": {
                "level": "INFO",
                "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            },
            "network": {
                "verify_ssl": False,  # ç¦ç”¨SSLéªŒè¯
                "allow_redirects": True,
                "max_redirects": 5
            }
        }
    
    def get(self, key_path: str, default=None):
        """è·å–é…ç½®é¡¹ï¼Œæ”¯æŒç‚¹å·åˆ†éš”çš„è·¯å¾„"""
        try:
            keys = key_path.split('.')
            value = self.config
            
            for key in keys:
                if isinstance(value, dict) and key in value:
                    value = value[key]
                else:
                    return default
            
            return value
        except Exception as e:
            logger.error(f"è·å–é…ç½®é¡¹å¤±è´¥: {key_path} - {e}")
            return default

class FixedWebScraper:
    """ä¿®å¤ç‰ˆç½‘ç»œçˆ¬è™«"""
    
    def __init__(self, config_manager: FixedConfigManager):
        self.config = config_manager
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = self.config.get('target_platform.headers', {})
        if headers:
            self.session.headers.update(headers)
        
        # è·å–ç½‘ç»œé…ç½®
        self.verify_ssl = self.config.get('network.verify_ssl', False)
        self.allow_redirects = self.config.get('network.allow_redirects', True)
        self.max_redirects = self.config.get('network.max_redirects', 5)
        
        # è·å–çˆ¬è™«é…ç½®
        self.base_url = self.config.get('target_platform.base_url')
        self.selectors = self.config.get('target_platform.selectors', {})
        self.max_retries = self.config.get('scraping.max_retries', 3)
        self.retry_delay = self.config.get('scraping.retry_delay', 2)
        self.request_timeout = self.config.get('scraping.request_timeout', 15)
        self.request_delay_min = self.config.get('scraping.request_delay_min', 2.0)
        self.request_delay_max = self.config.get('scraping.request_delay_max', 4.0)
        self.page_delay_min = self.config.get('scraping.page_delay_min', 3.0)
        self.page_delay_max = self.config.get('scraping.page_delay_max', 5.0)
        self.max_pages = self.config.get('scraping.max_pages', 1)
        
        # çˆ†æ¬¾æ ‡å‡†
        self.min_read_count = self.config.get('bestseller_criteria.min_read_count', 100)
        self.min_interaction_count = self.config.get('bestseller_criteria.min_interaction_count', 10)
        
        logger.info(f"ä¿®å¤ç‰ˆçˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ç›®æ ‡URL: {self.base_url}")
        logger.info(f"SSLéªŒè¯: {self.verify_ssl}")
        logger.info(f"è¶…æ—¶è®¾ç½®: {self.request_timeout}s")
        
    def test_connection(self, url: str = None) -> Dict:
        """æµ‹è¯•ç½‘ç»œè¿æ¥"""
        if url is None:
            url = self.base_url
            
        logger.info(f"æµ‹è¯•ç½‘ç»œè¿æ¥: {url}")
        result = {
            'success': False,
            'status_code': None,
            'headers': None,
            'content_length': 0,
            'error': None,
            'response_time': 0
        }
        
        try:
            start_time = time.time()
            response = self.session.get(
                url, 
                timeout=self.request_timeout,
                verify=self.verify_ssl,
                allow_redirects=self.allow_redirects
            )
            end_time = time.time()
            
            result['response_time'] = end_time - start_time
            result['status_code'] = response.status_code
            result['headers'] = dict(response.headers)
            result['content_length'] = len(response.text)
            result['success'] = True
            
            logger.info(f"è¿æ¥æµ‹è¯•æˆåŠŸ - çŠ¶æ€ç : {response.status_code}, å“åº”æ—¶é—´: {result['response_time']:.2f}s")
            
            if response.status_code != 200:
                logger.warning(f"é200çŠ¶æ€ç : {response.status_code}")
                
        except requests.exceptions.SSLError as e:
            result['error'] = f"SSLé”™è¯¯: {str(e)}"
            logger.error(f"SSLè¿æ¥å¤±è´¥: {e}")
            logger.info("å»ºè®®: è®¾ç½® verify_ssl=False æˆ–æ£€æŸ¥è¯ä¹¦é…ç½®")
            
        except requests.exceptions.ConnectionError as e:
            result['error'] = f"è¿æ¥é”™è¯¯: {str(e)}"
            logger.error(f"ç½‘ç»œè¿æ¥å¤±è´¥: {e}")
            logger.info("å»ºè®®: æ£€æŸ¥ç½‘ç»œè¿æ¥ã€ä»£ç†è®¾ç½®æˆ–ç›®æ ‡ç½‘ç«™æ˜¯å¦å¯è®¿é—®")
            
        except requests.exceptions.Timeout as e:
            result['error'] = f"è¶…æ—¶é”™è¯¯: {str(e)}"
            logger.error(f"è¯·æ±‚è¶…æ—¶: {e}")
            logger.info("å»ºè®®: å¢åŠ  request_timeout å€¼æˆ–æ£€æŸ¥ç½‘ç»œçŠ¶å†µ")
            
        except requests.exceptions.RequestException as e:
            result['error'] = f"è¯·æ±‚å¼‚å¸¸: {str(e)}"
            logger.error(f"è¯·æ±‚å¤±è´¥: {e}")
            
        except Exception as e:
            result['error'] = f"æœªé¢„æœŸé”™è¯¯: {str(e)}"
            logger.error(f"è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
            logger.error(traceback.format_exc())
        
        return result
    
    def make_request(self, url: str, timeout: int = None) -> Optional[requests.Response]:
        """å‘é€HTTPè¯·æ±‚ï¼ŒåŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†"""
        if timeout is None:
            timeout = self.request_timeout
            
        logger.info(f"å¼€å§‹è¯·æ±‚: {url}")
        
        for attempt in range(self.max_retries):
            try:
                logger.debug(f"è¯·æ±‚å°è¯• {attempt + 1}/{self.max_retries}: {url}")
                start_time = time.time()
                
                response = self.session.get(
                    url,
                    timeout=timeout,
                    verify=self.verify_ssl,
                    allow_redirects=self.allow_redirects
                )

                if not response.encoding or response.encoding.lower() == "iso-8859-1":
                    try:
                        response.encoding = response.apparent_encoding or "utf-8"
                    except Exception:
                        response.encoding = "utf-8"

                response_time = time.time() - start_time
                logger.info(f"è¯·æ±‚æˆåŠŸ - çŠ¶æ€ç : {response.status_code}, å“åº”æ—¶é—´: {response_time:.2f}s, URL: {url}")
                
                if response.status_code != 200:
                    logger.warning(f"é200çŠ¶æ€ç : {response.status_code}")
                return response
                
            except requests.exceptions.SSLError as e:
                logger.warning(f"SSLé”™è¯¯ (å°è¯• {attempt + 1}/{self.max_retries}): {url} - {e}")
                if attempt < self.max_retries - 1:
                    delay = self.retry_delay * (2 ** attempt) + random.uniform(0, 1)
                    logger.info(f"ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                else:
                    logger.error(f"SSLè¿æ¥æœ€ç»ˆå¤±è´¥: {url}")
                    return None
                    
            except requests.exceptions.ConnectionError as e:
                logger.warning(f"è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{self.max_retries}): {url} - {e}")
                if attempt < self.max_retries - 1:
                    delay = self.retry_delay * (2 ** attempt) + random.uniform(0, 1)
                    logger.info(f"ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                else:
                    logger.error(f"ç½‘ç»œè¿æ¥æœ€ç»ˆå¤±è´¥: {url}")
                    return None
                    
            except requests.exceptions.Timeout as e:
                logger.warning(f"è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{self.max_retries}): {url} - {e}")
                if attempt < self.max_retries - 1:
                    delay = self.retry_delay * (2 ** attempt) + random.uniform(0, 1)
                    logger.info(f"ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                else:
                    logger.error(f"è¯·æ±‚æœ€ç»ˆè¶…æ—¶: {url}")
                    return None
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"è¯·æ±‚å¼‚å¸¸ (å°è¯• {attempt + 1}/{self.max_retries}): {url} - {e}")
                logger.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
                if attempt < self.max_retries - 1:
                    delay = self.retry_delay * (2 ** attempt) + random.uniform(0, 1)
                    logger.info(f"ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                else:
                    logger.error(f"è¯·æ±‚æœ€ç»ˆå¤±è´¥: {url}")
                    return None
                    
            except Exception as e:
                logger.error(f"æœªé¢„æœŸçš„å¼‚å¸¸ (å°è¯• {attempt + 1}/{self.max_retries}): {url} - {e}")
                logger.error(traceback.format_exc())
                return None
        
        return None
    
    def fetch_article_links(self, page_url: str) -> List[str]:
        """è·å–æ–‡ç« åˆ—è¡¨é¡µä¸­çš„è¯¦æƒ…é¡µé“¾æ¥"""
        logger.info(f"å¼€å§‹è·å–æ–‡ç« åˆ—è¡¨: {page_url}")

        response = self.make_request(page_url)
        if not response:
            logger.error(f"è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {page_url}")
            if hasattr(self, "log_message"):
                try:
                    self.log_message(f"è·å–åˆ—è¡¨é¡µå¤±è´¥ï¼ˆç½‘ç»œ/åçˆ¬/è¶…æ—¶ï¼‰ï¼š{page_url}")
                except Exception:
                    pass
            return []

        try:
            content_type = response.headers.get("Content-Type", "")
            html_len = len(response.text or "")
            logger.info(f"åˆ—è¡¨é¡µå“åº”: status={response.status_code}, content-type={content_type}, html_len={html_len}")
            if hasattr(self, "log_message"):
                try:
                    self.log_message(f"åˆ—è¡¨é¡µå“åº”: {response.status_code}, HTMLé•¿åº¦: {html_len}")
                except Exception:
                    pass

            soup = BeautifulSoup(response.text, 'html.parser')
            logger.debug(f"é¡µé¢HTMLé•¿åº¦: {len(response.text)} å­—ç¬¦")

            a_count = len(soup.find_all("a"))
            logger.info(f"é¡µé¢è§£æç»Ÿè®¡: aæ ‡ç­¾æ•°é‡={a_count}")
            if hasattr(self, "log_message"):
                try:
                    self.log_message(f"é¡µé¢è§£æç»Ÿè®¡: aæ ‡ç­¾æ•°é‡={a_count}")
                except Exception:
                    pass
            
            selector = self.selectors.get('article_links', 'a')
            logger.debug(f"ä½¿ç”¨é€‰æ‹©å™¨: {selector}")
            
            links = []
            elements = soup.select(selector)
            logger.info(f"é€‰æ‹©å™¨æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
            if hasattr(self, "log_message"):
                try:
                    self.log_message(f"é€‰æ‹©å™¨å‘½ä¸­å…ƒç´ æ•°: {len(elements)}ï¼ˆselector={selector}ï¼‰")
                except Exception:
                    pass
            
            for i, a in enumerate(elements):
                href = a.get('href')
                text = a.get_text(strip=True)
                
                logger.debug(f"é“¾æ¥ {i+1}: href={href}, text={text[:50]}...")
                
                if href:
                    href = str(href).strip()
                    if href:
                        href = href.split()[0]

                    if href.startswith("https:/") and not href.startswith("https://"):
                        href = "https://" + href[len("https:/"):]
                    if href.startswith("http:/") and not href.startswith("http://"):
                        href = "http://" + href[len("http:/"):]

                    lower_href = href.lower()
                    if any(lower_href.endswith(ext) for ext in (".apk", ".jpg", ".jpeg", ".png", ".gif", ".css", ".js", ".pdf", ".zip")):
                        continue

                    # æ™ºèƒ½è¿‡æ»¤
                    text_len = len(text)
                    href_len = len(href)
                    
                    # è¿‡æ»¤è§„åˆ™
                    if text_len < 4:  # æ ‡é¢˜å¤ªçŸ­
                        logger.debug(f"è·³è¿‡é“¾æ¥(æ ‡é¢˜å¤ªçŸ­): {text[:10]}... - {href[:30]}...")
                        continue
                    if href_len < 10:  # é“¾æ¥å¤ªçŸ­
                        logger.debug(f"è·³è¿‡é“¾æ¥(URLå¤ªçŸ­): {href}")
                        continue
                    if href.startswith('javascript') or href.startswith('#'):
                        logger.debug(f"è·³è¿‡é“¾æ¥(æ— æ•ˆåè®®): {href}")
                        continue
                    
                    # æ’é™¤å¸¸è§éæ–°é—»é“¾æ¥
                    exclude_keywords = ['ç™»å½•', 'æ³¨å†Œ', 'å¸®åŠ©', 'å…³äº', 'è”ç³»', 'åé¦ˆ', 'æ›´å¤š', 'é¦–é¡µ', 'åœ°å›¾', 'æ‹›è˜']
                    if any(kw in text for kw in exclude_keywords):
                        logger.debug(f"è·³è¿‡é“¾æ¥(å…³é”®è¯æ’é™¤): {text}")
                        continue

                    # å¤„ç†ç›¸å¯¹URL
                    full_url = urljoin(page_url, href)
                    links.append(full_url)
                    logger.debug(f"æ·»åŠ é“¾æ¥: {full_url}")
                else:
                    logger.warning(f"é“¾æ¥ {i+1} æ²¡æœ‰hrefå±æ€§")

                if i < 5 and hasattr(self, "log_message"):
                    try:
                        self.log_message(f"æ ·ä¾‹é“¾æ¥{i+1}: {text[:30]} | {str(href)[:80]}")
                    except Exception:
                        pass
            
            logger.info(f"æ‰¾åˆ° {len(links)} ä¸ªæœ‰æ•ˆæ–‡ç« é“¾æ¥")
            if hasattr(self, "log_message"):
                try:
                    self.log_message(f"æœ‰æ•ˆæ–‡ç« é“¾æ¥æ•°: {len(links)}")
                except Exception:
                    pass
            
            if not links:
                logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆé“¾æ¥ï¼Œé€‰æ‹©å™¨å¯èƒ½ä¸æ­£ç¡®: {selector}")
                # æ˜¾ç¤ºé¡µé¢ç»“æ„å¸®åŠ©è°ƒè¯•
                self.show_page_structure_help(soup)
                if a_count == 0:
                    logger.info(f"é¡µé¢æ²¡æœ‰aæ ‡ç­¾ï¼Œå°†æŠŠå½“å‰é¡µå½“ä½œå•ç¯‡æ–‡ç« å¤„ç†: {page_url}")
                    if hasattr(self, "log_message"):
                        try:
                            self.log_message(f"é¡µé¢æ— é“¾æ¥ï¼ŒæŒ‰å•ç¯‡æ–‡ç« å¤„ç†: {page_url}")
                        except Exception:
                            pass
                    return [page_url]
            
            return links
            
        except Exception as e:
            logger.error(f"è§£æåˆ—è¡¨é¡µå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return []
    
    def show_page_structure_help(self, soup: BeautifulSoup):
        """æ˜¾ç¤ºé¡µé¢ç»“æ„å¸®åŠ©ä¿¡æ¯"""
        logger.info("é¡µé¢ç»“æ„åˆ†æå¸®åŠ©:")
        
        # æ˜¾ç¤ºæ‰€æœ‰é“¾æ¥
        all_links = soup.find_all('a', href=True)
        logger.info(f"é¡µé¢ä¸­æ€»å…±æœ‰ {len(all_links)} ä¸ªå¸¦hrefçš„é“¾æ¥")
        
        if len(all_links) <= 10:  # åªæ˜¾ç¤ºå°‘é‡é“¾æ¥
            for i, link in enumerate(all_links):
                href = link.get('href', '')
                text = link.get_text(strip=True)
                classes = link.get('class', [])
                link_id = link.get('id', '')
                
                logger.info(f"  é“¾æ¥ {i+1}: href='{href}' text='{text[:30]}...' class={classes} id='{link_id}'")
        
        # æ˜¾ç¤ºå¸¸è§å…ƒç´ 
        common_elements = {
            'h1': soup.find_all('h1'),
            'h2': soup.find_all('h2'),
            'h3': soup.find_all('h3'),
            'p': soup.find_all('p')[:5],  # åªæ˜¾ç¤ºå‰5ä¸ª
            'div': soup.find_all('div')[:5]
        }
        
        logger.info("å¸¸è§å…ƒç´ åˆ†æ:")
        for tag, elements in common_elements.items():
            if elements:
                logger.info(f"  {tag}æ ‡ç­¾: {len(elements)}ä¸ª")
                for i, elem in enumerate(elements[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    text = elem.get_text(strip=True)
                    classes = elem.get('class', [])
                    elem_id = elem.get('id', '')
                    logger.info(f"    {tag} {i+1}: text='{text[:30]}...' class={classes} id='{elem_id}'")
    
    def parse_article_detail(self, detail_url: str) -> Optional[Dict]:
        """è§£æå•ç¯‡æ–‡ç« è¯¦æƒ…é¡µï¼Œæå–å…³é”®ä¿¡æ¯"""
        logger.info(f"å¼€å§‹è§£ææ–‡ç« è¯¦æƒ…: {detail_url}")
        
        response = self.make_request(detail_url)
        if not response:
            logger.error(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {detail_url}")
            return {
                'title': '',
                'author': '',
                'publish_time': '',
                'read_count': 0,
                'like_count': 0,
                'collect_count': 0,
                'summary': '',
                'content': '',
                'detail_url': detail_url,
                'is_bestseller': False,
                'status_code': None,
                'error': 'request_failed'
            }
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            logger.debug(f"è¯¦æƒ…é¡µHTMLé•¿åº¦: {len(response.text)} å­—ç¬¦")

            # æå–æ•°æ®
            title = self.extract_text(soup, 'title')
            if not title:
                fallback_title = (soup.title.string or "").strip() if soup.title and soup.title.string else ""
                title = fallback_title
            author = self.extract_text(soup, 'author')
            publish_time = self.extract_text(soup, 'publish_time')
            read_count = self.extract_number(soup, 'read_count')
            like_count = self.extract_number(soup, 'like_count')
            collect_count = self.extract_number(soup, 'collect_count')
            content_summary = self.extract_text(soup, 'summary', max_length=200)
            content = self.extract_content(soup)
            if not content_summary and content:
                content_summary = content[:200] + ("..." if len(content) > 200 else "")
            
            logger.debug(f"æå–çš„æ•°æ® - æ ‡é¢˜: {title[:50]}..., ä½œè€…: {author}, å‘å¸ƒæ—¶é—´: {publish_time}")
            logger.debug(f"ç»Ÿè®¡æ•°æ® - é˜…è¯»: {read_count}, ç‚¹èµ: {like_count}, æ”¶è—: {collect_count}")
            
            # æ•°æ®éªŒè¯
            if not title:
                title = detail_url
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºçˆ†æ¬¾
            interaction_count = like_count + collect_count
            is_bestseller = self.is_bestseller(read_count, interaction_count)
            
            logger.info(f"æ–‡ç« åˆ†æ - æ ‡é¢˜: {title[:30]}..., é˜…è¯»é‡: {read_count}, äº’åŠ¨: {interaction_count}, çˆ†æ¬¾: {is_bestseller}")
            
            article_data = {
                'title': title,
                'author': author,
                'publish_time': publish_time,
                'read_count': read_count,
                'like_count': like_count,
                'collect_count': collect_count,
                'summary': content_summary,
                'content': content,
                'detail_url': detail_url,
                'is_bestseller': is_bestseller,
                'status_code': response.status_code,
                'error': None if response.status_code == 200 else f"http_{response.status_code}"
            }
            
            return article_data
            
        except Exception as e:
            logger.error(f"è§£æè¯¦æƒ…é¡µå¤±è´¥ {detail_url}: {e}")
            logger.error(traceback.format_exc())
            return None
    
    def extract_text(self, soup: BeautifulSoup, field: str, max_length: int = None) -> str:
        """æå–æ–‡æœ¬å†…å®¹"""
        selector = self.selectors.get(field)
        if not selector:
            logger.warning(f"æœªæ‰¾åˆ°å­—æ®µ {field} çš„é€‰æ‹©å™¨é…ç½®")
            return ''

    def extract_content(self, soup: BeautifulSoup) -> str:
        for t in soup.find_all(["script", "style", "noscript"]):
            t.decompose()

        selector = self.selectors.get('content')
        if selector:
            try:
                elem = soup.select_one(selector)
                if elem:
                    text = elem.get_text("\n", strip=True)
                    return text
            except Exception:
                pass

        article = soup.find("article")
        if article:
            paragraphs = [p.get_text(" ", strip=True) for p in article.find_all("p")]
            paragraphs = [p for p in paragraphs if len(p) >= 20]
            text = "\n".join(paragraphs).strip()
            if len(text) >= 200:
                return text

        paragraphs = [p.get_text(" ", strip=True) for p in soup.find_all("p")]
        paragraphs = [p for p in paragraphs if len(p) >= 20]
        text = "\n".join(paragraphs).strip()
        if len(text) >= 200:
            return text

        candidates = soup.find_all(["div", "section", "main"])
        best_text = ""
        for node in candidates:
            node_text = node.get_text("\n", strip=True)
            if len(node_text) > len(best_text):
                best_text = node_text
        return best_text.strip()
        
        try:
            element = soup.select_one(selector)
            if element:
                text = element.text.strip()
                if max_length and len(text) > max_length:
                    text = text[:max_length] + '...'
                logger.debug(f"æå–æ–‡æœ¬ - {field}: {selector} -> {text[:50]}...")
                return text
            else:
                logger.debug(f"æœªæ‰¾åˆ°å…ƒç´  - {field}: {selector}")
                return ''
        except Exception as e:
            logger.error(f"æå–æ–‡æœ¬å¤±è´¥ - {field}: {selector} - {e}")
            return ''
    
    def extract_number(self, soup: BeautifulSoup, field: str) -> int:
        """æå–æ•°å­—å†…å®¹"""
        selector = self.selectors.get(field)
        if not selector:
            logger.warning(f"æœªæ‰¾åˆ°å­—æ®µ {field} çš„é€‰æ‹©å™¨é…ç½®")
            return 0
        
        try:
            element = soup.select_one(selector)
            if element:
                text = element.text.strip().replace(',', '')
                # å°è¯•æå–æ•°å­—
                import re
                numbers = re.findall(r'\d+', text)
                if numbers:
                    number = int(numbers[0])
                    logger.debug(f"æå–æ•°å­— - {field}: {selector} -> {text} -> {number}")
                    return number
                else:
                    logger.debug(f"æœªæ‰¾åˆ°æ•°å­— - {field}: {selector} -> {text}")
                    return 0
            else:
                logger.debug(f"æœªæ‰¾åˆ°æ•°å­—å…ƒç´  - {field}: {selector}")
                return 0
        except (ValueError, AttributeError) as e:
            logger.warning(f"æ•°å­—è½¬æ¢å¤±è´¥ - {field}: {selector} - {e}")
            return 0
        except Exception as e:
            logger.error(f"æå–æ•°å­—å¤±è´¥ - {field}: {selector} - {e}")
            return 0
    
    def is_bestseller(self, read_count: int, interaction_count: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºçˆ†æ¬¾æ–‡ç« """
        result = (read_count > self.min_read_count) and (interaction_count > self.min_interaction_count)
        logger.debug(f"çˆ†æ¬¾åˆ¤æ–­ - é˜…è¯»: {read_count} > {self.min_read_count} = {read_count > self.min_read_count}, "
                    f"äº’åŠ¨: {interaction_count} > {self.min_interaction_count} = {interaction_count > self.min_interaction_count}, "
                    f"ç»“æœ: {result}")
        return result
    
    def fetch_multiple_pages(self, base_url: str = None, max_pages: int = None) -> List[str]:
        """æŠ“å–å¤šé¡µæ–‡ç« é“¾æ¥"""
        if base_url is None:
            base_url = self.base_url
        if max_pages is None:
            max_pages = self.max_pages
            
        logger.info(f"å¼€å§‹æŠ“å–å¤šé¡µæ–‡ç« ï¼ŒåŸºç¡€URL: {base_url}, æœ€å¤§é¡µæ•°: {max_pages}")
        all_links = []
        
        for page in range(1, max_pages + 1):
            logger.info(f"æŠ“å–ç¬¬ {page} é¡µ")
            
            # æ ¹æ®å®é™…ç½‘ç«™çš„ç¿»é¡µURLæ ¼å¼è°ƒæ•´
            if page > 1:
                sep = "&" if "?" in base_url else "?"
                page_url = f"{base_url}{sep}page={page}"
            else:
                page_url = base_url
            logger.debug(f"ç¬¬ {page} é¡µURL: {page_url}")
            
            links = self.fetch_article_links(page_url)
            if not links:
                logger.warning(f"ç¬¬ {page} é¡µæ— æ–‡ç« ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€é¡µ")
                continue
                
            all_links.extend(links)
            logger.info(f"ç¬¬ {page} é¡µè·å–åˆ° {len(links)} ä¸ªé“¾æ¥ï¼Œæ€»è®¡: {len(all_links)}")
            
            # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°
            delay = random.uniform(self.page_delay_min, self.page_delay_max)
            logger.debug(f"ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
            time.sleep(delay)
            
        logger.info(f"å¤šé¡µæŠ“å–å®Œæˆï¼Œæ€»å…±è·å– {len(all_links)} ç¯‡æ–‡ç« é“¾æ¥")
        return all_links


def save_to_csv(data: List[Dict], filename: str, encoding: str = 'utf-8-sig') -> bool:
    """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
    logger.info(f"å¼€å§‹ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶: {filename}")
    
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Path(filename).parent.mkdir(parents=True, exist_ok=True)
        
        default_fieldnames = [
            "title",
            "author",
            "publish_time",
            "read_count",
            "like_count",
            "collect_count",
            "summary",
            "content",
            "detail_url",
            "is_bestseller",
            "status_code",
            "error",
        ]

        fieldnames = list(data[0].keys()) if data else default_fieldnames

        logger.debug(f"æ•°æ®æ¡æ•°: {len(data)}")
        logger.debug(f"æ•°æ®å­—æ®µ: {fieldnames}")
        if data:
            logger.debug(f"æ•°æ®ç¤ºä¾‹: {data[0]}")
        
        with open(filename, mode='w', newline='', encoding=encoding) as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            if data:
                writer.writerows(data)
        
        logger.info(f"æˆåŠŸä¿å­˜ {len(data)} æ¡è®°å½•åˆ° {filename}")
        return True
        
    except Exception as e:
        logger.error(f"ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return False


def run_safe_test():
    """è¿è¡Œå®‰å…¨æµ‹è¯•"""
    print("=" * 60)
    print("ğŸ•·ï¸ æ°‘å•†æ³•çˆ†æ¬¾æ–‡ç« çˆ¬è™« - ä¿®å¤ç‰ˆå®‰å…¨æµ‹è¯•")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        config_manager = FixedConfigManager()
        logger.info("é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–çˆ¬è™«
        scraper = FixedWebScraper(config_manager)
        logger.info("ä¿®å¤ç‰ˆçˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        
        # 1. æµ‹è¯•ç½‘ç»œè¿æ¥
        print("\n1ï¸âƒ£ æµ‹è¯•ç½‘ç»œè¿æ¥...")
        connection_test = scraper.test_connection()
        if connection_test['success']:
            print(f"âœ… ç½‘ç»œè¿æ¥æ­£å¸¸")
            print(f"   çŠ¶æ€ç : {connection_test['status_code']}")
            print(f"   å“åº”æ—¶é—´: {connection_test['response_time']:.2f}ç§’")
            print(f"   å†…å®¹é•¿åº¦: {connection_test['content_length']}å­—ç¬¦")
        else:
            print(f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥")
            print(f"   é”™è¯¯: {connection_test['error']}")
            return False
        
        # 2. è¿è¡Œå°è§„æ¨¡çˆ¬å–æµ‹è¯•
        print("\n2ï¸âƒ£ è¿è¡Œå°è§„æ¨¡çˆ¬å–æµ‹è¯•...")
        article_links = scraper.fetch_multiple_pages(max_pages=1)
        print(f"âœ… çˆ¬å–æµ‹è¯•å®Œæˆ")
        print(f"   è·å–æ–‡ç« é“¾æ¥: {len(article_links)}ä¸ª")
        
        if article_links:
            print(f"   å‰3ä¸ªé“¾æ¥:")
            for i, link in enumerate(article_links[:3]):
                print(f"     {i+1}. {link}")
            
            # æµ‹è¯•è§£æç¬¬ä¸€ä¸ªæ–‡ç« 
            print(f"\n   æµ‹è¯•è§£æç¬¬ä¸€ä¸ªæ–‡ç« ...")
            first_article = scraper.parse_article_detail(article_links[0])
            if first_article:
                print(f"âœ… æ–‡ç« è§£ææˆåŠŸ")
                print(f"   æ ‡é¢˜: {first_article['title'][:50]}...")
                print(f"   ä½œè€…: {first_article['author']}")
                print(f"   é˜…è¯»é‡: {first_article['read_count']}")
                print(f"   æ˜¯å¦ä¸ºçˆ†æ¬¾: {first_article['is_bestseller']}")
            else:
                print(f"âš ï¸ æ–‡ç« è§£æå¤±è´¥æˆ–ä¸ç¬¦åˆçˆ†æ¬¾æ ‡å‡†")
        else:
            print(f"âš ï¸ æœªè·å–åˆ°ä»»ä½•æ–‡ç« é“¾æ¥")
        
        # 3. ä¿å­˜ç»“æœ
        if article_links:
            csv_filename = config_manager.get('output.csv_filename', 'fixed_bestsellers.csv')
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_data = []
            for i, link in enumerate(article_links[:5]):  # åªå–å‰5ä¸ª
                article_data = scraper.parse_article_detail(link)
                if article_data:
                    test_data.append(article_data)
            
            if test_data:
                if save_to_csv(test_data, csv_filename):
                    print(f"\nâœ… ç»“æœä¿å­˜æˆåŠŸ")
                    print(f"   æ–‡ä»¶: {csv_filename}")
                    print(f"   è®°å½•æ•°: {len(test_data)}")
                else:
                    print(f"\nâŒ ç»“æœä¿å­˜å¤±è´¥")
            else:
                print(f"\nâš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ä¿å­˜")
        
        print("\n" + "=" * 60)
        print("ğŸ” ä¿®å¤ç‰ˆæµ‹è¯•å®Œæˆï¼")
        print(f"   æ—¥å¿—æ–‡ä»¶: fixed_scraper.log")
        print(f"   ç»“æœæ–‡ä»¶: {config_manager.get('output.csv_filename')}")
        print("=" * 60)
        
        return True
        
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        return False
    except Exception as e:
        print(f"\n\næµ‹è¯•è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
        logger.error(traceback.format_exc())
        return False


def main():
    """ä¸»å‡½æ•°"""
    try:
        success = run_safe_test()
        if success:
            print("\nâœ… ä¿®å¤ç‰ˆçˆ¬è™«æµ‹è¯•æˆåŠŸï¼")
            print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
            print("   1. ä¿®æ”¹ config.json ä¸­çš„ base_url ä¸ºæ‚¨è¦çˆ¬å–çš„çœŸå®ç½‘ç«™")
            print("   2. æ ¹æ®ç›®æ ‡ç½‘ç«™ç»“æ„è°ƒæ•´ selectors é…ç½®")
            print("   3. è°ƒæ•´ bestseller_criteria ä¸­çš„æ ‡å‡†å€¼")
            print("   4. è¿è¡Œ python configurable_scraper.py å¼€å§‹æ­£å¼çˆ¬å–")
        else:
            print("\nâŒ ä¿®å¤ç‰ˆçˆ¬è™«æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶")
            
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nç¨‹åºå‘ç”Ÿé”™è¯¯: {e}")
        logger.error(traceback.format_exc())


if __name__ == '__main__':
    main()
