#!/usr/bin/env python3
"""
æ°‘å•†æ³•çˆ†æ¬¾æ–‡ç« çˆ¬è™« - è°ƒè¯•ç‰ˆæœ¬
ç”¨äºè¯Šæ–­å’Œè§£å†³çˆ¬è™«è¿è¡Œä¸­çš„é—®é¢˜
"""

import requests
from bs4 import BeautifulSoup
import time
import csv
import logging
import json
from typing import List, Dict, Optional
import random
from urllib.parse import urljoin, urlparse
from pathlib import Path
import traceback

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,  # è®¾ç½®ä¸ºDEBUGçº§åˆ«ä»¥è·å–æ›´å¤šä¿¡æ¯
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('debug_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DebugConfigManager:
    """è°ƒè¯•é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_file: str = 'config.json'):
        self.config_file = config_file
        self.config = self.load_config()
        
    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶ï¼ŒåŒ…å«è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯"""
        try:
            if not os.path.exists(self.config_file):
                logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_file}ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
                return self.get_default_config()
                
            with open(self.config_file, 'r', encoding='utf-8') as f:
                content = f.read()
                logger.debug(f"é…ç½®æ–‡ä»¶å†…å®¹: {content[:200]}...")
                return json.loads(content)
        except json.JSONDecodeError as e:
            logger.error(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            logger.error(f"é”™è¯¯ä½ç½®: è¡Œ {e.lineno}, åˆ— {e.colno}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {e.msg}")
            return self.get_default_config()
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return self.get_default_config()
    
    def get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        logger.info("ä½¿ç”¨é»˜è®¤é…ç½®")
        return {
            "target_platform": {
                "base_url": "https://example-law-platform.com/civil-commercial",
                "headers": {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                },
                "selectors": {
                    "article_links": "a.article-link",
                    "title": "h1.article-title",
                    "author": ".author-name",
                    "publish_time": ".publish-date",
                    "read_count": ".read-count",
                    "like_count": ".like-count",
                    "collect_count": ".collect-count",
                    "summary": ".article-summary"
                }
            },
            "scraping": {
                "max_pages": 1,  # è°ƒè¯•æ—¶åªæŠ“å–1é¡µ
                "max_retries": 3,
                "retry_delay": 2,
                "request_timeout": 10,
                "request_delay_min": 1.0,
                "request_delay_max": 2.0,
                "page_delay_min": 2.0,
                "page_delay_max": 3.0
            },
            "bestseller_criteria": {
                "min_read_count": 1000,  # è°ƒè¯•æ—¶é™ä½æ ‡å‡†
                "min_interaction_count": 100
            },
            "output": {
                "csv_filename": "debug_bestsellers.csv",
                "encoding": "utf-8-sig",
                "log_filename": "debug_scraper.log"
            },
            "logging": {
                "level": "DEBUG",
                "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            }
        }
    
    def get(self, key_path: str, default=None):
        """è·å–é…ç½®é¡¹ï¼Œæ”¯æŒç‚¹å·åˆ†éš”çš„è·¯å¾„"""
        try:
            keys = key_path.split('.')
            value = self.config
            
            for key in keys:
                if isinstance(value, dict) and key in value:
                    value = value[key]
                else:
                    logger.debug(f"é…ç½®é¡¹æœªæ‰¾åˆ°: {key_path}ï¼Œè¿”å›é»˜è®¤å€¼: {default}")
                    return default
            
            logger.debug(f"è·å–é…ç½®é¡¹: {key_path} = {value}")
            return value
        except Exception as e:
            logger.error(f"è·å–é…ç½®é¡¹å¤±è´¥: {key_path} - {e}")
            return default

class DebugWebScraper:
    """è°ƒè¯•ç‰ˆç½‘ç»œçˆ¬è™«"""
    
    def __init__(self, config_manager: DebugConfigManager):
        self.config = config_manager
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = self.config.get('target_platform.headers', {})
        if headers:
            self.session.headers.update(headers)
            logger.debug(f"è®¾ç½®è¯·æ±‚å¤´: {headers}")
        else:
            logger.warning("æœªè®¾ç½®è¯·æ±‚å¤´ï¼Œä½¿ç”¨é»˜è®¤è¯·æ±‚å¤´")
        
        # è·å–é…ç½®å‚æ•°
        self.base_url = self.config.get('target_platform.base_url')
        self.selectors = self.config.get('target_platform.selectors', {})
        self.max_retries = self.config.get('scraping.max_retries', 3)
        self.retry_delay = self.config.get('scraping.retry_delay', 2)
        self.request_timeout = self.config.get('scraping.request_timeout', 10)
        self.request_delay_min = self.config.get('scraping.request_delay_min', 0.5)
        self.request_delay_max = self.config.get('scraping.request_delay_max', 2.0)
        self.page_delay_min = self.config.get('scraping.page_delay_min', 1.0)
        self.page_delay_max = self.config.get('scraping.page_delay_max', 3.0)
        self.max_pages = self.config.get('scraping.max_pages', 1)
        
        # çˆ†æ¬¾æ ‡å‡†
        self.min_read_count = self.config.get('bestseller_criteria.min_read_count', 10000)
        self.min_interaction_count = self.config.get('bestseller_criteria.min_interaction_count', 1000)
        
        logger.info(f"çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œç›®æ ‡URL: {self.base_url}")
        logger.info(f"é€‰æ‹©å™¨é…ç½®: {self.selectors}")
        
    def test_connection(self, url: str = None) -> Dict:
        """æµ‹è¯•ç½‘ç»œè¿æ¥"""
        if url is None:
            url = self.base_url
            
        logger.info(f"æµ‹è¯•ç½‘ç»œè¿æ¥: {url}")
        result = {
            'success': False,
            'status_code': None,
            'headers': None,
            'content_length': 0,
            'error': None,
            'response_time': 0
        }
        
        try:
            start_time = time.time()
            response = self.session.get(url, timeout=self.request_timeout)
            end_time = time.time()
            
            result['response_time'] = end_time - start_time
            result['status_code'] = response.status_code
            result['headers'] = dict(response.headers)
            result['content_length'] = len(response.text)
            result['success'] = True
            
            logger.info(f"è¿æ¥æµ‹è¯•æˆåŠŸ - çŠ¶æ€ç : {response.status_code}, å“åº”æ—¶é—´: {result['response_time']:.2f}s, å†…å®¹é•¿åº¦: {result['content_length']}")
            
            # æ£€æŸ¥å“åº”å†…å®¹
            if response.status_code == 200:
                logger.debug(f"å“åº”å†…å®¹å‰200å­—ç¬¦: {response.text[:200]}...")
            else:
                logger.warning(f"HTTPçŠ¶æ€ç å¼‚å¸¸: {response.status_code}")
                
        except requests.exceptions.RequestException as e:
            result['error'] = str(e)
            logger.error(f"è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
            logger.error(traceback.format_exc())
        
        return result
    
    def analyze_page_structure(self, url: str) -> Dict:
        """åˆ†æé¡µé¢ç»“æ„ï¼Œå¸®åŠ©é€‰æ‹©å™¨é…ç½®"""
        logger.info(f"åˆ†æé¡µé¢ç»“æ„: {url}")
        result = {
            'success': False,
            'title': None,
            'all_links': [],
            'article_links': [],
            'suggested_selectors': {},
            'error': None
        }
        
        try:
            response = self.session.get(url, timeout=self.request_timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            result['title'] = soup.title.string if soup.title else None
            
            # è·å–æ‰€æœ‰é“¾æ¥
            all_links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                text = link.get_text(strip=True)
                all_links.append({
                    'href': href,
                    'text': text,
                    'class': link.get('class', []),
                    'id': link.get('id', '')
                })
            result['all_links'] = all_links
            
            # æ™ºèƒ½è¯†åˆ«æ–‡ç« é“¾æ¥
            article_links = []
            common_patterns = ['article', 'post', 'news', 'blog', 'content']
            
            for link in all_links:
                href = link['href'].lower()
                text = link['text'].lower()
                link_classes = [cls.lower() for cls in link['class']] if link['class'] else []
                link_id = link['id'].lower()
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…å¸¸è§æ¨¡å¼
                if any(pattern in href or pattern in text or any(pattern in cls for cls in link_classes) or pattern in link_id for pattern in common_patterns):
                    article_links.append(link)
            
            result['article_links'] = article_links
            
            # ç”Ÿæˆå»ºè®®çš„é€‰æ‹©å™¨
            suggested_selectors = {}
            if article_links:
                # åŸºäºclassçš„å»ºè®®
                class_counts = {}
                for link in article_links:
                    for cls in link['class']:
                        class_counts[cls] = class_counts.get(cls, 0) + 1
                
                if class_counts:
                    most_common_class = max(class_counts, key=class_counts.get)
                    suggested_selectors['article_links'] = f"a.{most_common_class}"
                
                # åŸºäºå…¶ä»–å…ƒç´ çš„å»ºè®®
                suggested_selectors['title'] = "h1, h2, .title, .post-title, .article-title"
                suggested_selectors['author'] = ".author, .byline, .writer, .post-author"
                suggested_selectors['publish_time'] = ".date, .time, .publish-date, .post-date"
                suggested_selectors['read_count'] = ".read, .views, .read-count, .view-count"
                suggested_selectors['like_count'] = ".like, .likes, .thumb, .vote"
                suggested_selectors['collect_count'] = ".collect, .bookmark, .favorite"
                suggested_selectors['summary'] = ".summary, .excerpt, .description, .post-content"
            
            result['suggested_selectors'] = suggested_selectors
            result['success'] = True
            
            logger.info(f"é¡µé¢åˆ†æå®Œæˆ - æ ‡é¢˜: {result['title']}")
            logger.info(f"å‘ç°æ–‡ç« é“¾æ¥: {len(article_links)} ä¸ª")
            logger.info(f"å»ºè®®çš„é€‰æ‹©å™¨: {suggested_selectors}")
            
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"é¡µé¢ç»“æ„åˆ†æå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
        
        return result
    
    def test_selectors(self, url: str, selectors: Dict) -> Dict:
        """æµ‹è¯•é€‰æ‹©å™¨æ˜¯å¦æœ‰æ•ˆ"""
        logger.info(f"æµ‹è¯•é€‰æ‹©å™¨: {url}")
        result = {
            'success': False,
            'selector_results': {},
            'error': None
        }
        
        try:
            response = self.session.get(url, timeout=self.request_timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            selector_results = {}
            
            for field, selector in selectors.items():
                try:
                    elements = soup.select(selector)
                    selector_results[field] = {
                        'selector': selector,
                        'found_elements': len(elements),
                        'sample_text': elements[0].get_text(strip=True) if elements else None,
                        'status': 'found' if elements else 'not_found'
                    }
                    
                    if elements:
                        logger.info(f"é€‰æ‹©å™¨æµ‹è¯•æˆåŠŸ - {field}: {selector} (æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ )")
                        if len(elements) > 0:
                            logger.debug(f"ç¤ºä¾‹å†…å®¹: {elements[0].get_text(strip=True)[:100]}...")
                    else:
                        logger.warning(f"é€‰æ‹©å™¨æœªæ‰¾åˆ°å…ƒç´  - {field}: {selector}")
                        
                except Exception as e:
                    selector_results[field] = {
                        'selector': selector,
                        'found_elements': 0,
                        'error': str(e),
                        'status': 'error'
                    }
                    logger.error(f"é€‰æ‹©å™¨æµ‹è¯•é”™è¯¯ - {field}: {selector} - {e}")
            
            result['selector_results'] = selector_results
            result['success'] = True
            
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"é€‰æ‹©å™¨æµ‹è¯•å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
        
        return result
    
    def make_request(self, url: str, timeout: int = None) -> Optional[requests.Response]:
        """å‘é€HTTPè¯·æ±‚ï¼ŒåŒ…å«è¯¦ç»†çš„é”™è¯¯å¤„ç†"""
        if timeout is None:
            timeout = self.request_timeout
            
        logger.info(f"å¼€å§‹è¯·æ±‚: {url}")
        
        for attempt in range(self.max_retries):
            try:
                logger.debug(f"è¯·æ±‚å°è¯• {attempt + 1}/{self.max_retries}: {url}")
                start_time = time.time()
                
                response = self.session.get(url, timeout=timeout)
                response_time = time.time() - start_time
                
                logger.info(f"è¯·æ±‚æˆåŠŸ - çŠ¶æ€ç : {response.status_code}, å“åº”æ—¶é—´: {response_time:.2f}s, URL: {url}")
                
                if response.status_code != 200:
                    logger.warning(f"é200çŠ¶æ€ç : {response.status_code} for {url}")
                
                response.raise_for_status()
                return response
                
            except requests.exceptions.Timeout as e:
                logger.warning(f"è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{self.max_retries}): {url} - {e}")
                if attempt < self.max_retries - 1:
                    delay = self.retry_delay * (2 ** attempt) + random.uniform(0, 1)
                    logger.info(f"ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                else:
                    logger.error(f"è¯·æ±‚æœ€ç»ˆè¶…æ—¶: {url}")
                    return None
                    
            except requests.exceptions.ConnectionError as e:
                logger.warning(f"è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{self.max_retries}): {url} - {e}")
                if attempt < self.max_retries - 1:
                    delay = self.retry_delay * (2 ** attempt) + random.uniform(0, 1)
                    logger.info(f"ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                else:
                    logger.error(f"è¿æ¥æœ€ç»ˆå¤±è´¥: {url}")
                    return None
                    
            except requests.exceptions.HTTPError as e:
                logger.error(f"HTTPé”™è¯¯: {url} - {e}")
                logger.error(f"å“åº”çŠ¶æ€ç : {response.status_code if 'response' in locals() else 'æœªçŸ¥'}")
                return None
                
            except requests.exceptions.RequestException as e:
                logger.error(f"è¯·æ±‚å¼‚å¸¸ (å°è¯• {attempt + 1}/{self.max_retries}): {url} - {e}")
                logger.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
                if attempt < self.max_retries - 1:
                    delay = self.retry_delay * (2 ** attempt) + random.uniform(0, 1)
                    logger.info(f"ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                else:
                    logger.error(f"è¯·æ±‚æœ€ç»ˆå¤±è´¥: {url}")
                    return None
                    
            except Exception as e:
                logger.error(f"æœªé¢„æœŸçš„å¼‚å¸¸ (å°è¯• {attempt + 1}/{self.max_retries}): {url} - {e}")
                logger.error(traceback.format_exc())
                return None
        
        return None
    
    def fetch_article_links(self, page_url: str) -> List[str]:
        """è·å–æ–‡ç« åˆ—è¡¨é¡µä¸­çš„è¯¦æƒ…é¡µé“¾æ¥ - è°ƒè¯•ç‰ˆæœ¬"""
        logger.info(f"å¼€å§‹è·å–æ–‡ç« åˆ—è¡¨: {page_url}")
        
        response = self.make_request(page_url)
        if not response:
            logger.error(f"è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥: {page_url}")
            return []
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            logger.debug(f"é¡µé¢HTMLé•¿åº¦: {len(response.text)} å­—ç¬¦")
            
            selector = self.selectors.get('article_links', 'a.article-link')
            logger.debug(f"ä½¿ç”¨é€‰æ‹©å™¨: {selector}")
            
            links = []
            elements = soup.select(selector)
            logger.info(f"é€‰æ‹©å™¨æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
            
            for i, a in enumerate(elements):
                href = a.get('href')
                text = a.get_text(strip=True)
                
                logger.debug(f"é“¾æ¥ {i+1}: href={href}, text={text[:50]}...")
                
                if href:
                    # å¤„ç†ç›¸å¯¹URL
                    full_url = urljoin(page_url, href)
                    links.append(full_url)
                    logger.debug(f"æ·»åŠ é“¾æ¥: {full_url}")
                else:
                    logger.warning(f"é“¾æ¥ {i+1} æ²¡æœ‰hrefå±æ€§")
            
            logger.info(f"æ‰¾åˆ° {len(links)} ä¸ªæœ‰æ•ˆæ–‡ç« é“¾æ¥")
            
            if not links:
                logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆé“¾æ¥ï¼Œé€‰æ‹©å™¨å¯èƒ½ä¸æ­£ç¡®: {selector}")
                # å°è¯•åˆ†æé¡µé¢ç»“æ„
                structure_analysis = self.analyze_page_structure(page_url)
                if structure_analysis['success'] and structure_analysis['suggested_selectors']:
                    logger.info(f"å»ºè®®å°è¯•çš„é€‰æ‹©å™¨: {structure_analysis['suggested_selectors']}")
            
            return links
            
        except Exception as e:
            logger.error(f"è§£æåˆ—è¡¨é¡µå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return []
    
    def parse_article_detail(self, detail_url: str) -> Optional[Dict]:
        """è§£æå•ç¯‡æ–‡ç« è¯¦æƒ…é¡µï¼Œæå–å…³é”®ä¿¡æ¯ - è°ƒè¯•ç‰ˆæœ¬"""
        logger.info(f"å¼€å§‹è§£ææ–‡ç« è¯¦æƒ…: {detail_url}")
        
        response = self.make_request(detail_url)
        if not response:
            logger.error(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {detail_url}")
            return None
        
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            logger.debug(f"è¯¦æƒ…é¡µHTMLé•¿åº¦: {len(response.text)} å­—ç¬¦")
            
            # æå–æ•°æ®
            title = self.extract_text(soup, 'title')
            author = self.extract_text(soup, 'author')
            publish_time = self.extract_text(soup, 'publish_time')
            read_count = self.extract_number(soup, 'read_count')
            like_count = self.extract_number(soup, 'like_count')
            collect_count = self.extract_number(soup, 'collect_count')
            content_summary = self.extract_text(soup, 'summary', max_length=200)
            
            logger.debug(f"æå–çš„æ•°æ® - æ ‡é¢˜: {title[:50]}..., ä½œè€…: {author}, å‘å¸ƒæ—¶é—´: {publish_time}")
            logger.debug(f"ç»Ÿè®¡æ•°æ® - é˜…è¯»: {read_count}, ç‚¹èµ: {like_count}, æ”¶è—: {collect_count}")
            
            # æ•°æ®éªŒè¯
            if not title:
                logger.warning(f"æ–‡ç« æ ‡é¢˜ä¸ºç©ºï¼Œè·³è¿‡: {detail_url}")
                return None
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºçˆ†æ¬¾
            interaction_count = like_count + collect_count
            is_bestseller = self.is_bestseller(read_count, interaction_count)
            
            logger.info(f"æ–‡ç« åˆ†æ - æ ‡é¢˜: {title[:30]}..., é˜…è¯»é‡: {read_count}, äº’åŠ¨: {interaction_count}, çˆ†æ¬¾: {is_bestseller}")
            
            article_data = {
                'title': title,
                'author': author,
                'publish_time': publish_time,
                'read_count': read_count,
                'like_count': like_count,
                'collect_count': collect_count,
                'summary': content_summary,
                'detail_url': detail_url,
                'is_bestseller': is_bestseller
            }
            
            return article_data if is_bestseller else None
            
        except Exception as e:
            logger.error(f"è§£æè¯¦æƒ…é¡µå¤±è´¥ {detail_url}: {e}")
            logger.error(traceback.format_exc())
            return None
    
    def extract_text(self, soup: BeautifulSoup, field: str, max_length: int = None) -> str:
        """æå–æ–‡æœ¬å†…å®¹ - è°ƒè¯•ç‰ˆæœ¬"""
        selector = self.selectors.get(field)
        if not selector:
            logger.warning(f"æœªæ‰¾åˆ°å­—æ®µ {field} çš„é€‰æ‹©å™¨é…ç½®")
            return ''
        
        try:
            element = soup.select_one(selector)
            if element:
                text = element.text.strip()
                if max_length and len(text) > max_length:
                    text = text[:max_length] + '...'
                logger.debug(f"æå–æ–‡æœ¬ - {field}: {selector} -> {text[:50]}...")
                return text
            else:
                logger.debug(f"æœªæ‰¾åˆ°å…ƒç´  - {field}: {selector}")
                return ''
        except Exception as e:
            logger.error(f"æå–æ–‡æœ¬å¤±è´¥ - {field}: {selector} - {e}")
            return ''
    
    def extract_number(self, soup: BeautifulSoup, field: str) -> int:
        """æå–æ•°å­—å†…å®¹ - è°ƒè¯•ç‰ˆæœ¬"""
        selector = self.selectors.get(field)
        if not selector:
            logger.warning(f"æœªæ‰¾åˆ°å­—æ®µ {field} çš„é€‰æ‹©å™¨é…ç½®")
            return 0
        
        try:
            element = soup.select_one(selector)
            if element:
                text = element.text.strip().replace(',', '')
                number = int(text) if text.isdigit() else 0
                logger.debug(f"æå–æ•°å­— - {field}: {selector} -> {text} -> {number}")
                return number
            else:
                logger.debug(f"æœªæ‰¾åˆ°æ•°å­—å…ƒç´  - {field}: {selector}")
                return 0
        except (ValueError, AttributeError) as e:
            logger.warning(f"æ•°å­—è½¬æ¢å¤±è´¥ - {field}: {selector} - {e}")
            return 0
        except Exception as e:
            logger.error(f"æå–æ•°å­—å¤±è´¥ - {field}: {selector} - {e}")
            return 0
    
    def is_bestseller(self, read_count: int, interaction_count: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºçˆ†æ¬¾æ–‡ç«  - è°ƒè¯•ç‰ˆæœ¬"""
        result = (read_count > self.min_read_count) and (interaction_count > self.min_interaction_count)
        logger.debug(f"çˆ†æ¬¾åˆ¤æ–­ - é˜…è¯»: {read_count} > {self.min_read_count} = {read_count > self.min_read_count}, "
                    f"äº’åŠ¨: {interaction_count} > {self.min_interaction_count} = {interaction_count > self.min_interaction_count}, "
                    f"ç»“æœ: {result}")
        return result
    
    def fetch_multiple_pages(self, base_url: str = None, max_pages: int = None) -> List[str]:
        """æŠ“å–å¤šé¡µæ–‡ç« é“¾æ¥ - è°ƒè¯•ç‰ˆæœ¬"""
        if base_url is None:
            base_url = self.base_url
        if max_pages is None:
            max_pages = self.max_pages
            
        logger.info(f"å¼€å§‹æŠ“å–å¤šé¡µæ–‡ç« ï¼ŒåŸºç¡€URL: {base_url}, æœ€å¤§é¡µæ•°: {max_pages}")
        all_links = []
        
        for page in range(1, max_pages + 1):
            logger.info(f"æŠ“å–ç¬¬ {page} é¡µ")
            
            # æ ¹æ®å®é™…ç½‘ç«™çš„ç¿»é¡µURLæ ¼å¼è°ƒæ•´
            page_url = f"{base_url}?page={page}" if page > 1 else base_url
            logger.debug(f"ç¬¬ {page} é¡µURL: {page_url}")
            
            links = self.fetch_article_links(page_url)
            if not links:
                logger.warning(f"ç¬¬ {page} é¡µæ— æ–‡ç« ï¼Œåœæ­¢ç¿»é¡µ")
                break
                
            all_links.extend(links)
            logger.info(f"ç¬¬ {page} é¡µè·å–åˆ° {len(links)} ä¸ªé“¾æ¥ï¼Œæ€»è®¡: {len(all_links)}")
            
            # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°
            delay = random.uniform(self.page_delay_min, self.page_delay_max)
            logger.debug(f"ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
            time.sleep(delay)
            
        logger.info(f"å¤šé¡µæŠ“å–å®Œæˆï¼Œæ€»å…±è·å– {len(all_links)} ç¯‡æ–‡ç« é“¾æ¥")
        return all_links


def debug_save_to_csv(data: List[Dict], filename: str, encoding: str = 'utf-8-sig') -> bool:
    """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶ - è°ƒè¯•ç‰ˆæœ¬"""
    logger.info(f"å¼€å§‹ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶: {filename}")
    
    if not data:
        logger.warning("æ— æ•°æ®å¯ä¿å­˜")
        return False
    
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Path(filename).parent.mkdir(parents=True, exist_ok=True)
        
        logger.debug(f"æ•°æ®æ¡æ•°: {len(data)}")
        if data:
            logger.debug(f"æ•°æ®å­—æ®µ: {list(data[0].keys())}")
            logger.debug(f"æ•°æ®ç¤ºä¾‹: {data[0]}")
        
        with open(filename, mode='w', newline='', encoding=encoding) as f:
            writer = csv.DictWriter(f, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)
        
        logger.info(f"æˆåŠŸä¿å­˜ {len(data)} æ¡è®°å½•åˆ° {filename}")
        return True
        
    except Exception as e:
        logger.error(f"ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return False


def run_diagnostics():
    """è¿è¡Œå®Œæ•´çš„è¯Šæ–­æµ‹è¯•"""
    print("=" * 60)
    print("ğŸ•·ï¸ æ°‘å•†æ³•çˆ†æ¬¾æ–‡ç« çˆ¬è™« - è¯Šæ–­æ¨¡å¼")
    print("=" * 60)
    
    # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
    config_manager = DebugConfigManager()
    logger.info("é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # åˆå§‹åŒ–çˆ¬è™«
    scraper = DebugWebScraper(config_manager)
    logger.info("è°ƒè¯•çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    # 1. æµ‹è¯•ç½‘ç»œè¿æ¥
    print("\n1ï¸âƒ£ æµ‹è¯•ç½‘ç»œè¿æ¥...")
    connection_test = scraper.test_connection()
    if connection_test['success']:
        print(f"âœ… ç½‘ç»œè¿æ¥æ­£å¸¸")
        print(f"   çŠ¶æ€ç : {connection_test['status_code']}")
        print(f"   å“åº”æ—¶é—´: {connection_test['response_time']:.2f}ç§’")
        print(f"   å†…å®¹é•¿åº¦: {connection_test['content_length']}å­—ç¬¦")
    else:
        print(f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥")
        print(f"   é”™è¯¯: {connection_test['error']}")
        return False
    
    # 2. åˆ†æé¡µé¢ç»“æ„
    print("\n2ï¸âƒ£ åˆ†æé¡µé¢ç»“æ„...")
    structure_analysis = scraper.analyze_page_structure(scraper.base_url)
    if structure_analysis['success']:
        print(f"âœ… é¡µé¢ç»“æ„åˆ†æå®Œæˆ")
        print(f"   é¡µé¢æ ‡é¢˜: {structure_analysis['title']}")
        print(f"   æ€»é“¾æ¥æ•°: {len(structure_analysis['all_links'])}")
        print(f"   æ–‡ç« é“¾æ¥æ•°: {len(structure_analysis['article_links'])}")
        
        if structure_analysis['suggested_selectors']:
            print(f"   å»ºè®®çš„é€‰æ‹©å™¨:")
            for field, selector in structure_analysis['suggested_selectors'].items():
                print(f"     {field}: {selector}")
    else:
        print(f"âŒ é¡µé¢ç»“æ„åˆ†æå¤±è´¥")
        print(f"   é”™è¯¯: {structure_analysis['error']}")
    
    # 3. æµ‹è¯•å½“å‰é€‰æ‹©å™¨
    print("\n3ï¸âƒ£ æµ‹è¯•å½“å‰é€‰æ‹©å™¨é…ç½®...")
    selector_test = scraper.test_selectors(scraper.base_url, scraper.selectors)
    if selector_test['success']:
        print(f"âœ… é€‰æ‹©å™¨æµ‹è¯•å®Œæˆ")
        for field, result in selector_test['selector_results'].items():
            status_icon = "âœ…" if result['status'] == 'found' else "âŒ"
            print(f"   {status_icon} {field}: {result['selector']}")
            print(f"      æ‰¾åˆ°å…ƒç´ : {result['found_elements']}ä¸ª")
            if result['sample_text']:
                print(f"      ç¤ºä¾‹å†…å®¹: {result['sample_text'][:50]}...")
            if result['status'] == 'error':
                print(f"      é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    else:
        print(f"âŒ é€‰æ‹©å™¨æµ‹è¯•å¤±è´¥")
        print(f"   é”™è¯¯: {selector_test['error']}")
    
    # 4. è¿è¡Œå°è§„æ¨¡çˆ¬å–æµ‹è¯•
    print("\n4ï¸âƒ£ è¿è¡Œå°è§„æ¨¡çˆ¬å–æµ‹è¯•...")
    try:
        # åªæŠ“å–ä¸€é¡µï¼Œé™ä½æ ‡å‡†
        article_links = scraper.fetch_multiple_pages(max_pages=1)
        print(f"âœ… çˆ¬å–æµ‹è¯•å®Œæˆ")
        print(f"   è·å–æ–‡ç« é“¾æ¥: {len(article_links)}ä¸ª")
        
        if article_links:
            print(f"   å‰3ä¸ªé“¾æ¥:")
            for i, link in enumerate(article_links[:3]):
                print(f"     {i+1}. {link}")
            
            # æµ‹è¯•è§£æç¬¬ä¸€ä¸ªæ–‡ç« 
            print(f"\n   æµ‹è¯•è§£æç¬¬ä¸€ä¸ªæ–‡ç« ...")
            first_article = scraper.parse_article_detail(article_links[0])
            if first_article:
                print(f"âœ… æ–‡ç« è§£ææˆåŠŸ")
                print(f"   æ ‡é¢˜: {first_article['title'][:50]}...")
                print(f"   ä½œè€…: {first_article['author']}")
                print(f"   é˜…è¯»é‡: {first_article['read_count']}")
                print(f"   æ˜¯å¦ä¸ºçˆ†æ¬¾: {first_article['is_bestseller']}")
            else:
                print(f"âš ï¸ æ–‡ç« è§£æå¤±è´¥æˆ–ä¸ç¬¦åˆçˆ†æ¬¾æ ‡å‡†")
        else:
            print(f"âš ï¸ æœªè·å–åˆ°ä»»ä½•æ–‡ç« é“¾æ¥")
            
    except Exception as e:
        print(f"âŒ çˆ¬å–æµ‹è¯•å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
    
    print("\n" + "=" * 60)
    print("ğŸ” è¯Šæ–­å®Œæˆï¼è¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯:")
    print(f"   æ—¥å¿—æ–‡ä»¶: debug_scraper.log")
    print(f"   ç»“æœæ–‡ä»¶: debug_bestsellers.csv")
    print("=" * 60)
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    try:
        run_diagnostics()
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­è¯Šæ–­")
    except Exception as e:
        print(f"\n\nè¯Šæ–­è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
        logger.error(traceback.format_exc())


if __name__ == '__main__':
    main()